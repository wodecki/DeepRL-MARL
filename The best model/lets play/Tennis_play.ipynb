{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DRLND: Collaboration and Competition - let the trained models play\n",
        "\n",
        "---\n",
        "\n",
        "OK, the models are trained... Now let's see them playing.\n",
        "\n### 1. Start the Environment"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from unityagents import UnityEnvironment\n",
        "import numpy as np\n",
        "\n",
        "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
        "\n",
        "# get the default brain\n",
        "brain_name = env.brain_names[0]\n",
        "brain = env.brains[brain_name]\n",
        "\n",
        "# reset the environment\n",
        "env_info = env.reset(train_mode=True)[brain_name]\n",
        "\n",
        "# number of agents \n",
        "num_agents = len(env_info.agents)\n",
        "\n",
        "# size of each action\n",
        "action_size = brain.vector_action_space_size\n",
        "\n",
        "# examine the state space \n",
        "states = env_info.vector_observations\n",
        "state_size = states.shape[1]\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:unityagents:\n",
            "'Academy' started successfully!\n",
            "Unity Academy name: Academy\n",
            "        Number of Brains: 1\n",
            "        Number of External Brains : 1\n",
            "        Lesson number : 0\n",
            "        Reset Parameters :\n",
            "\t\t\n",
            "Unity brain name: TennisBrain\n",
            "        Number of Visual Observations (per agent): 0\n",
            "        Vector Observation space type: continuous\n",
            "        Vector Observation space size (per agent): 8\n",
            "        Number of stacked Vector Observation: 3\n",
            "        Vector Action space type: continuous\n",
            "        Vector Action space size (per agent): 2\n",
            "        Vector Action descriptions: , \n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import model\n",
        "import maddpg_agent\n",
        "import parameters\n",
        "import torch"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "state_size = 24\n",
        "action_size = 2\n",
        "TAU = 1e-2\n",
        "LR_ACTOR = 1e-3\n",
        "LR_CRITIC = 1e-4\n",
        "fc1_units = 300\n",
        "fc2_units = 400\n",
        "\n",
        "agents = []\n",
        "agent_1 = maddpg_agent.Agent(state_size, action_size, TAU, LR_ACTOR, LR_CRITIC, fc1_units, fc2_units, random_seed=123)\n",
        "agent_2 = maddpg_agent.Agent(state_size, action_size, TAU, LR_ACTOR, LR_CRITIC, fc1_units, fc2_units, random_seed=345)\n",
        "\n",
        "agent_1.actor_local.load_state_dict(torch.load('checkpoint_actor1_4.pth'))\n",
        "agent_2.actor_local.load_state_dict(torch.load('checkpoint_actor2_4.pth'))\n",
        "\nagents = [agent_1, agent_2]"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "scores = np.zeros(num_agents)\n",
        "while True:\n",
        "    actions = np.array([agents[i].act(states[i]) for i in range(num_agents)])\n",
        "\n",
        "    env_info = env.step(actions)[brain_name]        \n",
        "    next_states = env_info.vector_observations     \n",
        "    rewards = env_info.rewards                    \n",
        "    dones = env_info.local_done        \n",
        "\n",
        "    states = next_states\n",
        "    scores += rewards\n",
        "\n",
        "    print('\\rScores: {:.2f}\\t{:.2f}'\n",
        "              .format(scores[0], scores[1]), end=\"\") \n",
        "    \n",
        "    if np.any(dones):\n",
        "        break\n",
        "        \n",
        "print(\"\\nScores: {}\".format(scores))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scores: 2.60\t2.60\n",
            "Scores: [2.60000004 2.60000004]\n"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "env.close()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "drlnd",
      "language": "python",
      "display_name": "drlnd"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.2",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "drlnd"
    },
    "nteract": {
      "version": "0.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}